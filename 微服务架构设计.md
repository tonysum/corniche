# 微服务架构设计文档

## 当前架构分析

### 现状

当前系统采用**单体架构**，所有功能集成在一个 `api_server.py` 中：

```
api_server.py (单体服务)
├── 数据管理服务
│   ├── 数据下载 (/api/download)
│   ├── 数据查询 (/api/kline, /api/symbols)
│   ├── 数据完整性检查 (/api/data-integrity)
│   └── 数据修复 (/api/recheck-problematic-symbols)
├── 回测服务
│   └── 交易回测 (/api/backtest)
└── 工具服务
    └── 订单计算 (/api/calculate-order)
```

### 存在的问题

1. **扩展性问题**
   - 数据管理和回测服务耦合，无法独立扩展
   - 回测是 CPU 密集型，数据管理是 I/O 密集型，资源需求不同

2. **部署问题**
   - 任何功能更新都需要重新部署整个服务
   - 回测功能更新不影响数据管理，但必须一起部署

3. **资源竞争**
   - 长时间回测会占用资源，影响数据下载和管理
   - 无法为不同服务设置不同的资源限制

4. **故障隔离**
   - 一个服务故障可能影响整个系统
   - 回测崩溃可能导致数据管理也无法使用

5. **开发协作**
   - 多个开发者修改同一文件容易冲突
   - 代码耦合度高，难以独立测试

---

## 微服务架构方案

### 架构设计

```
┌─────────────────────────────────────────────────────────┐
│                     前端 (Next.js)                        │
│                  http://localhost:3000                     │
└──────────────────────┬────────────────────────────────────┘
                       │
                       │ HTTP/API Gateway
                       │
        ┌──────────────┼──────────────┐
        │              │              │
        ▼              ▼              ▼
┌──────────────┐ ┌──────────────┐ ┌──────────────┐
│  数据管理服务  │ │  回测服务     │ │  订单计算服务  │
│  Data Service │ │ Backtest     │ │ Order Service│
│  :8001        │ │ Service :8002 │ │ :8003        │
└──────┬────────┘ └──────┬───────┘ └──────┬───────┘
       │                  │                 │
       └──────────────────┼─────────────────┘
                          │
                          ▼
                  ┌───────────────┐
                  │  共享数据库    │
                  │  SQLite/MySQL │
                  └───────────────┘
```

### 服务拆分

#### 1. 数据管理服务 (Data Service)
- **端口**: 8001
- **职责**:
  - K线数据下载和管理
  - 数据查询和检索
  - 数据完整性检查
  - 数据修复和重检
- **资源需求**: I/O 密集型，需要网络和磁盘 I/O
- **扩展策略**: 水平扩展，可以部署多个实例处理下载任务

#### 2. 回测服务 (Backtest Service)
- **端口**: 8002
- **职责**:
  - 交易策略回测
  - 回测结果计算和统计
  - 回测历史记录管理
- **资源需求**: CPU 密集型，需要大量计算资源
- **扩展策略**: 可以部署多个实例并行处理多个回测任务

#### 3. 订单计算服务 (Order Service)
- **端口**: 8003
- **职责**:
  - 订单价格计算
  - 杠杆和保证金计算
  - 盈亏计算
- **资源需求**: CPU 密集型，但计算量较小
- **扩展策略**: 可以合并到回测服务，或独立部署

#### 4. API Gateway (可选)
- **端口**: 8000
- **职责**:
  - 统一入口
  - 路由转发
  - 负载均衡
  - 认证授权
- **技术选型**: Nginx, Kong, Traefik, 或自建 FastAPI Gateway

---

## 实施步骤

### 阶段1: 准备阶段（当前）

1. **代码模块化**
   - 将 `api_server.py` 拆分为多个模块
   - 提取共享代码到公共库
   - 定义服务间接口规范

2. **数据库准备**
   - 评估是否需要迁移到 MySQL/PostgreSQL
   - 或保持 SQLite，但做好数据访问抽象

### 阶段2: 服务拆分（渐进式）

#### 步骤1: 拆分数据管理服务

创建 `services/data_service/`:

```
services/data_service/
├── main.py              # FastAPI 应用
├── routes/
│   ├── download.py      # 数据下载路由
│   ├── query.py         # 数据查询路由
│   └── integrity.py     # 数据完整性路由
├── models.py            # 数据模型
└── Dockerfile
```

#### 步骤2: 拆分回测服务

创建 `services/backtest_service/`:

```
services/backtest_service/
├── main.py              # FastAPI 应用
├── routes/
│   └── backtest.py      # 回测路由
├── models.py            # 回测模型
└── Dockerfile
```

#### 步骤3: 拆分订单计算服务

创建 `services/order_service/`:

```
services/order_service/
├── main.py              # FastAPI 应用
├── routes/
│   └── calculate.py     # 订单计算路由
└── Dockerfile
```

### 阶段3: 部署和测试

1. **Docker Compose 配置**
   - 更新 `docker-compose.yml` 支持多服务
   - 配置服务间网络通信
   - 设置资源限制

2. **前端适配**
   - 更新前端 API 调用
   - 配置不同服务的 API 地址
   - 处理跨服务调用

---

## 微服务 Docker Compose 配置

### docker-compose.microservices.yml

```yaml
version: '3.8'

services:
  # API Gateway (可选)
  api-gateway:
    build:
      context: ./services/gateway
      dockerfile: Dockerfile
    container_name: crypto_gateway
    ports:
      - "8000:8000"
    environment:
      - DATA_SERVICE_URL=http://data-service:8001
      - BACKTEST_SERVICE_URL=http://backtest-service:8002
      - ORDER_SERVICE_URL=http://order-service:8003
    depends_on:
      - data-service
      - backtest-service
      - order-service
    networks:
      - crypto_network
    restart: unless-stopped

  # 数据管理服务
  data-service:
    build:
      context: ./services/data_service
      dockerfile: Dockerfile
    container_name: crypto_data_service
    ports:
      - "8001:8001"
    volumes:
      - ./db:/app/db
    environment:
      - PYTHONUNBUFFERED=1
      - PORT=8001
    networks:
      - crypto_network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # 回测服务
  backtest-service:
    build:
      context: ./services/backtest_service
      dockerfile: Dockerfile
    container_name: crypto_backtest_service
    ports:
      - "8002:8002"
    volumes:
      - ./db:/app/db
      - ./backtrade_records:/app/backtrade_records
    environment:
      - PYTHONUNBUFFERED=1
      - PORT=8002
      - DATA_SERVICE_URL=http://data-service:8001
    networks:
      - crypto_network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 4G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # 订单计算服务
  order-service:
    build:
      context: ./services/order_service
      dockerfile: Dockerfile
    container_name: crypto_order_service
    ports:
      - "8003:8003"
    environment:
      - PYTHONUNBUFFERED=1
      - PORT=8003
    networks:
      - crypto_network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # 前端服务
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: crypto_frontend
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=http://localhost:8000  # 使用 Gateway
      # 或直接使用各服务
      # - NEXT_PUBLIC_DATA_API_URL=http://localhost:8001
      # - NEXT_PUBLIC_BACKTEST_API_URL=http://localhost:8002
      # - NEXT_PUBLIC_ORDER_API_URL=http://localhost:8003
    depends_on:
      - api-gateway
    networks:
      - crypto_network
    restart: unless-stopped

networks:
  crypto_network:
    driver: bridge
```

---

## 服务间通信

### 方案1: HTTP REST API（推荐）

```python
# 回测服务调用数据服务
import httpx

async def get_kline_data(symbol: str, interval: str):
    async with httpx.AsyncClient() as client:
        response = await client.get(
            f"{DATA_SERVICE_URL}/api/kline/{interval}/{symbol}"
        )
        return response.json()
```

### 方案2: 消息队列（异步任务）

对于长时间运行的任务（如数据下载、回测），可以使用消息队列：

```python
# 使用 Celery + Redis/RabbitMQ
from celery import Celery

celery_app = Celery('backtest', broker='redis://redis:6379')

@celery_app.task
def run_backtest(params):
    # 执行回测
    pass
```

### 方案3: gRPC（高性能场景）

对于高频率调用，可以使用 gRPC：

```protobuf
// data_service.proto
service DataService {
  rpc GetKline(GetKlineRequest) returns (KlineResponse);
}
```

---

## 数据共享策略

### 方案1: 共享数据库（当前）

- **优点**: 简单，无需数据同步
- **缺点**: 服务耦合，难以独立扩展

### 方案2: 数据库拆分

```
数据管理服务 → crypto_data.db (K线数据)
回测服务 → backtest.db (回测记录)
```

- **优点**: 服务解耦，独立扩展
- **缺点**: 需要数据同步机制

### 方案3: 事件驱动架构

使用事件总线（如 RabbitMQ, Kafka）进行数据同步：

```
数据管理服务 → 数据更新事件 → 回测服务订阅事件
```

---

## 迁移策略

### 渐进式迁移（推荐）

1. **第一步**: 保持现有单体服务运行
2. **第二步**: 拆分数据管理服务，前端同时调用新旧服务
3. **第三步**: 拆分回测服务
4. **第四步**: 拆分订单计算服务
5. **第五步**: 移除单体服务，完全切换到微服务

### 蓝绿部署

1. 部署新的微服务架构（绿环境）
2. 保持旧服务运行（蓝环境）
3. 逐步切换流量到新服务
4. 验证无误后关闭旧服务

---

## 监控和日志

### 服务监控

- **Prometheus**: 指标收集
- **Grafana**: 可视化仪表板
- **Jaeger**: 分布式追踪

### 日志聚合

- **ELK Stack**: Elasticsearch + Logstash + Kibana
- **Loki**: 轻量级日志聚合
- **Fluentd**: 日志收集和转发

---

## 成本分析

### 单体架构
- **服务器**: 1 台（4核8G）
- **成本**: 低
- **扩展性**: 差

### 微服务架构
- **服务器**: 3-4 台（根据负载）
- **成本**: 中等（可以使用容器编排优化）
- **扩展性**: 优秀

### 优化建议

1. **使用 Kubernetes**: 自动扩缩容，资源利用率高
2. **容器化**: Docker 容器共享资源
3. **云服务**: 使用云服务商的容器服务（如 ECS, EKS）

---

## 何时拆分？

### 拆分时机

✅ **应该拆分**:
- 团队规模 > 5 人
- 服务需要独立部署和扩展
- 不同服务有不同的资源需求
- 需要故障隔离

❌ **暂不拆分**:
- 团队规模小（< 3 人）
- 业务简单，耦合度高
- 资源有限
- 快速迭代阶段

### 当前建议

**短期（1-3个月）**: 
- 保持单体架构
- 进行代码模块化重构
- 提取共享代码

**中期（3-6个月）**:
- 拆分数据管理服务
- 保持回测服务在单体中
- 评估拆分效果

**长期（6个月+）**:
- 完全拆分为微服务
- 引入 API Gateway
- 配置监控和日志系统

---

## 总结

### 当前架构适合的场景
- ✅ 小团队（1-3人）
- ✅ 业务简单
- ✅ 快速迭代
- ✅ 资源有限

### 微服务架构适合的场景
- ✅ 团队规模大（5+人）
- ✅ 业务复杂
- ✅ 需要独立扩展
- ✅ 需要故障隔离
- ✅ 不同服务有不同资源需求

### 建议

**现在**: 保持单体架构，但进行代码模块化重构

**未来**: 当出现以下情况时考虑拆分：
1. 团队规模扩大
2. 需要独立扩展数据管理和回测服务
3. 回测任务影响数据管理性能
4. 需要更细粒度的资源控制

---

**记住**: 不要过早优化！先让业务跑起来，再根据实际需求进行架构调整。

